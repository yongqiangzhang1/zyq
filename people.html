<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8" />
		<title>People</title>
		<meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no"/>
		<meta name="keywords" content="CVIS、内蒙古大学、内蒙古大学计算机学院、张永强研究组" />
		<meta name="description" content="内蒙古大学计算机学院张永强研究员科研团队">
		<link type="text/css" href="./font-awesome/css/font-awesome.min.css" rel="stylesheet">
		<link type="text/css" href="./css/basic.css" rel="stylesheet">
		<link type="text/css" href="./css/header.css" rel="stylesheet">
		<link type="text/css" href="./css/people.css" rel="stylesheet">
		<link type="text/css" href="./css/fooder.css" rel="stylesheet">
		<link rel="canonical" href="https://ttslr.github.io/">
		<script type='text/javascript' src="./js/jquery-3.3.1.min.js"></script>
		<link rel="stylesheet" href="./css/main_rl.css">
		<meta http-equiv="cleartype" content="on">
		<link rel="stylesheet" href="./css/academicons_rl.css" />
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); 
		</script>
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } }); 
		</script>
		<script>
			document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async>
		</script>
	</head>
	<body>
		<header class="pc_header">
			<div class="header_left">
				<img src="img/title.png"/>
				<!-- <span>语音生成与理解研究组</span> -->
			</div>
			<div class="header_right">
				<ul>
					<li id="element_1" onclick="window.location.href='./index.html'">
						<a>Home</a>
					</li>

					<li id="element_2" style="color: #015293;">
						<i class="fa fa-user" aria-hidden="true"></i>
						<a>People</a>
					</li>
					<li id="element_3" onclick="window.location.href='./publication.html'">
						<a>Publications</a>
					</li>
					
					
					
					<li id="element_5" onclick="window.location.href='./activity.html'">
						<a>Activity</a>
					</li>
					
					<li id="element_4" onclick="window.location.href='./joinUs.html'">
						<a>Join Us</a>
					</li>
				</ul>
			</div>
			
			
		</header>
		
		<main class="pc_people">
			<!-- <img src="img/4.jpg"/> -->
			
			<!--[if lt IE 9]><div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->
			<div class="masthead">
				<div class="masthead__inner-wrap">
					<div class="masthead__menu">
						<nav id="site-nav" class="greedy-nav">
							<button>
								<div class="navicon"></div>
							</button>
							<ul class="visible-links">
								<li class="masthead__menu-item">
									<!-- <a href="https://ttslr.github.io/index_ruiliu.html" onclick="myFunction(1)">Rui Liu 刘瑞</a> -->
									<a href="#" onclick="showModule(1)">Yongqiang Zhang (张永强)</a>
								</li>
								<li class="masthead__menu-item">
									<!-- <a href="https://ttslr.github.io/" onclick="myFunction(1)">S2LAB 团队网页</a> -->
									<a href="#" onclick="showModule(2)">Team Members</a>
								</li>
								<li class="masthead__menu-item">
									<!-- <a href="https://ttslr.github.io/" onclick="myFunction(2)">合作机构</a> -->
									<a href="#" onclick="showModule(3)">Collaborators</a>
								</li>
							</ul>
							<ul class="hidden-links hidden"></ul>
						</nav>
					</div>
				</div>
			</div>
			<div class="module" id="module1">
			    <div id="main" role="main">
			    	<!-- 左侧信息栏 -->
			    	<div class="sidebar sticky">
			    		<div itemscope itemtype="http://schema.org/Person">
			    			<div class="author__avatar">
			    				<img src="assets/YongqiangZhang.jpg" class="author__avatar" alt="Hanlei Zhang">
			    			</div>
			    			<div class="author__content">
			    				<h3 class="author__name">Yongqiang Zhang</h3>
			    
			    
			    				<p class="author__bio">Artificial Intelligence / Deep Learning, Computer Vision, Intelligent Testing.</p>
			    				<p class="author__bio">
			    					<!-- Office: 503 Room, School of Computer science, Inner Mongolia University, China 010021 -->
			    					Mobile: +86 15045651571
			    				</p>
			    			</div>
			    			<div class="author__urls-wrapper">
			    				<button class="btn btn--inverse">Follow</button>
			    				<ul class="author__urls social-icons">
			    					<li>
			    						<i class="fa fa-fw fa-map-marker" aria-hidden="true"></i>
			    						Inner Mongolia University, China
			    					</li>
			    					<li>
			    						<a href="mailto:zhangyongqiang@imu.edu.cn">
			    							<i class="fas fa-fw fa-envelope" aria-hidden="true"></i>
			    							Email
			    						</a>
			    					</li>
			    					<!-- <li>
			                            <a href="xxx">
			                                <i class="fab fa-fw fa-researchgate" aria-hidden="true"></i>
			                                ResearchGate
			                            </a>
			                        </li> -->
			    					<li>
			    						<a href="https://github.com/yongqiangzhang1">
			    							<i class="fab fa-fw fa-github" aria-hidden="true"></i>
			    							Github
			    						</a>
			    					</li>
			    					<li>
			    						<a href="https://scholar.google.com/citations?hl=zh-CN&user=mgpE1noAAAAJ&view_op=list_works&sortby=pubdate">
			    							<i class="fas fa-fw fa-graduation-cap"></i>
			    							Google Scholar
			    						</a>
			    					</li>
			    					<!-- <li>
			                            <a href="https://orcid.org/xxxxx">
			                                <i class="ai ai-orcid-square ai-fw"></i>
			                                ORCID
			                            </a>
			                        </li> -->
			    				</ul>
			    			</div>
			    		</div>
			    	</div>
			    	<article class="page" itemscope itemtype="http://schema.org/CreativeWork">
			    		<meta itemprop="headline" content="About me">
			    		<meta itemprop="description" content="About me">
			    		<div class="page__inner-wrap">
			    			<header>
			    				<h1 class="page__title" itemprop="headline">About me</h1>
			    			</header>
			    			<section class="page__content" itemprop="text">
			    				<p>
			    					Zhang Yongqiang, Ph.D., is a Research Fellow (Junma Program A3 Position) and a Ph.D. supervisor. He was selected for the “2020 National Postdoctoral Innovative Talent Support Program (Boxin Program)” and received the First Prize of the Outstanding Doctoral Dissertation Award from the Heilongjiang Provincial Artificial Intelligence Society.

His research focuses on artificial intelligence, deep learning/machine learning, and computer vision. His work covers high-level vision tasks (such as object detection and image segmentation) as well as low-level vision tasks (including image super-resolution, image deblurring, deraining, and dehazing). He has published more than 30 SCI/EI-indexed papers, including multiple articles in top-tier international journals (such as IJCV, TNNLS, and PR) and leading international conferences in artificial intelligence and computer vision (including CVPR, ICCV, ECCV, NeurIPS, and AAAI). He holds 11 authorized invention patents.

He has led several research projects, including the China Postdoctoral Science Foundation Innovative Talent Support Program, the National Natural Science Foundation of China (Young Scientists Fund), the Heilongjiang Provincial Postdoctoral General Program, the Heilongjiang Provincial Natural Science Foundation Joint Guidance Program, and multiple aerospace-related research projects. He also serves as a reviewer for several prestigious journals, including T-PAMI, TNNLS, PR, PRL, and SPL, as well as major international conferences such as CVPR, ICCV, ECCV, NeurIPS, and AAAI.
								
			    				</p>


							<p><a href="https://ccs.imu.edu.cn/info/1023/2401.htm">Chinese Version Personal Official Website</a></p>
			    
			    				<h2 id="education-and-experience">Education and Experience</h2>
								<ul>

									<li>
										Aug 2024–Present, Research Fellow & Ph.D. Supervisor
										<br>
										School of Computer Science, Inner Mongolia University, China.
									</li>

									<li>
										Sep 2020–Aug 2024, Lecturer & Master’s Supervisor
										<br>
										School of Instrumentation Science and Engineering, Harbin Institute of Technology, China.
									</li>

									<li>
										Jul 2020–Jul 2024, Postdoctoral Researcher (Co-advisor: Prof. Wangmeng Zuo)
										<br>
										School of Computer Science and Technology, Harbin Institute of Technology, China.
									</li>

									<li>
										Sep 2015–Apr 2020, Ph.D.
										<br>
										School of Instrumentation Science and Engineering, Harbin Institute of Technology, China.
									</li>

									<li>
										Mar 2017–Apr 2018, Visiting Ph.D. Student
										<br>
										Visual Computing Center, King Abdullah University of Science and Technology (KAUST), Saudi Arabia.
									</li>

									<li>
										Sep 2013–Jul 2015, M.E.
										<br>
										School of Electrical Engineering and Automation, Harbin Institute of Technology, China.
									</li>

									<li>
										Sep 2009–Jul 2013, B.E.
										<br>
										School of Measurement-Control Technology and Communication Engineering, Harbin University of Science and Technology, China.
									</li>

								</ul>
			    				 
			    				<h2 id="publication" class="page-header">Selected Publications <a
			    						href="https://scholar.google.com/citations?hl=zh-CN&user=mgpE1noAAAAJ&view_op=list_works&sortby=pubdate"
			    						target="_blank" rel="external">[Google Scholar]</a></h2>
			    				(#: equal contribution *: corresponding author)
<!-- 			    				<h3>Preprints</h3>
			    				<ol class="paper-list" id="grants">
			    					<li>
			    						<b>Controllable Accented Text-to-Speech Synthesis </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Guanglai Gao, Haizhou Li.<br>
			    						<font color="#1B58B8">To be submitted for possible journal publication <br></font>
			    						<font color="#1B58B8"><a href="https://arxiv.org/abs/2209.10804"
			    								target="_blank">[PDF]</a> <a
			    								href="https://speechdemo.github.io/caitts/">[DEMO]</a></font>
			    					</li>
			    					<li>
			    						<b>FCTalker: Fine and Coarse Grained Context Modeling for Expressive Conversational
			    							Speech Synthesis </b> <br>
			    						Yifan Hu, <b>Rui Liu <sup>*</sup></b>, Guanglai Gao, Haizhou Li.<br>
			    						 <font color="#1B58B8">Submitted to ICASSP'2023 <br></font> 
			    						<font color="#1B58B8"><a href="https://arxiv.org/abs/2210.15360"
			    								target="_blank">[PDF]</a> <a
			    								href="https://walker-hyf.github.io/FCTalker/">[DEMO]</a></font> <a
			    							href="https://github.com/walker-hyf/FCTalker">[CODE]</a></font>
			    					</li>
			    				</ol> !-->
<!-- 			    
			    				<h3>Journal papers</h3>
			    				<ol class="paper-list" id="grants">

								<li>
			    						<b>Contrastive Learning based Modality-Invariant Feature Acquisition for Robust Multimodal Emotion Recognition with Missing Modalities </b> <br>
			    						<b>Rui Liu</b>, Haolin Zuo, Zheng Lian, Bjorn W. Schuller and Haizhou Li.<br>
			    						<font color="#1B58B8">IEEE Transactions on Affective Computing
			    							(IEEE-TAC). 2024 <br></font>
			    						<font color="red">(Top journal, CAAI-A, IF=11.2)</font><br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/10474146/"
			    								target="_blank">[PDF]</a> <a href="https://github.com/ZhuoYulang/CIF-MMIN">[CODE]</a>
										
			    						</font>
			    					</li>


								<li>
			    						<b>Controllable Accented Text-to-Speech Synthesis with Fine and Coarse-Grained Intensity Rendering </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Guanglai Gao and Haizhou Li.<br>
			    						<font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing
			    							(IEEE/ACM-TASLP). 2024 <br></font>
			    						<font color="red">(Top journal, TH-CPL-A, IF=4.364)</font><br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/10487819"
			    								target="_blank">[PDF]</a> <a href="https://ttslr.github.io/CTA-TTS/">[DEMO]</a>
			    						</font>
			    					</li>


								<li>
			    						<b>Text-to-Speech for Low-Resource Agglutinative Language with Morphology-Aware Language Model Pre-training </b> <br>
			    						<b>Rui Liu</b>, Yifan Hu, Haolin Zuo, Zhaojie Luo, Longbiao Wang and Guanglai Gao.<br>
			    						<font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing
			    							(IEEE/ACM-TASLP). 2024 <br></font>
			    						<font color="red">(Top journal, TH-CPL-A, IF=4.364)</font><br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/10379131"
			    								target="_blank">[PDF]</a> <a href="https://ttslr.github.io/MAM-BERT/">[DEMO]</a>
			    						</font>
			    					</li>

								<li>
			    						<b>Multi-Space Channel Representation Learning for Mono-to-Binaural Conversion based Audio Deepfake Detection </b> <br>
			    						<b>Rui Liu</b>, Jinhua Zhang and Guanglai Gao.<br>
			    						<font color="#1B58B8">Information Fusion. 2024 <br></font>
			    						<font color="red">(Top journal, JCR Q1, IF=18.6)</font><br>
			    						<font color="#1B58B8"><a href="https://www.sciencedirect.com/science/article/pii/S1566253524000356"
			    								target="_blank">[PDF]</a>
			    						</font>
			    					</li>


								
			    					<li>
			    						<b>Decoupling Speaker-Independent Emotions for Voice Conversion Via Source-Filter
			    							Networks </b> <br>
			    						Zhaojie Luo, Shoufeng Lin, <b>Rui Liu <sup>*</sup></b>, Jun Baba, Yuichiro Yoshikawa,
			    						Ishiguro Hiroshi.<br>
			    						<font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing
			    							(IEEE/ACM-TASLP). 2022 <br></font>
			    						<font color="red">(Top journal, JCR Q1, IF=4.364)</font><br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9829916"
			    								target="_blank">[PDF]</a> <a href="https://zhaojiel.github.io/SFEVC/">[DEMO]</a>
			    						</font>
			    					</li>
			    					<li>
			    						<b>Decoding Knowledge Transfer for Neural Text-to-Speech Training</b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Guanglai Gao, Haizhou Li.<br>
			    						<font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing
			    							(IEEE/ACM-TASLP). 2022 <br></font>
			    						<font color="red">(Top journal, JCR Q1, IF=4.364)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9767637"
			    								target="_blank">[PDF]</a> <a href="https://ttslr.github.io/MT-KD/">[DEMO]</a> <a
			    								href="./papers/TASLP2022-decoding.txt" target="_blank">[BIB]</a></font>
			    					</li>
			    					<li>
			    						<b>Multi-Stage Deep Transfer Learning for EmIoT-enabled Human-Computer Interaction</b>
			    						<br>
			    						<b>Rui Liu</b>, Qi Liu, Hongxu Zhu, Hui Cao.<br>
			    						<font color="#1B58B8">IEEE Internet of Things Journal. 2022 </font><br> </font>
			    						<font color="red">(Top journal, JCR Q1, IF=10.238)</font><br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9702532"
			    								target="_blank">[PDF]</a> <a href="https://ttslr.github.io/IOT/">[DEMO]</a>
			    						</font>
			    					</li>
			    					<li>
			    						<b>MonTTS: A Real-time and High-fidelity Mongolian TTS Model with Complete
			    							Non-autoregressive Mechanism (in Chinese) </b> <br>
			    						<b>Rui Liu</b>, Shyin Kang, Jingdong Li, Feilong Bao, Guanglai Gao.<br>
			    						<font color="#1B58B8">Journal of Chinese Information Processing. 2022 </font><br>
			    						</font>
			    						<font color="red">(CCF T1)</font><br>
			    						<font color="#1B58B8"><a href="http://jcip.cipsc.org.cn/CN/abstract/abstract3357.shtml"
			    								target="_blank">[PDF]</a> <a href="https://github.com/ttslr/MonTTS">[CODE]</a>
			    						</font>
			    					</li>
			    					<li>
			    						<b>Emotional Voice Conversion: Theory, Databases and ESD </b> <br>
			    						Kun Zhou, Berrak Sisman, <b>Rui Liu</b>, Haizhou Li.<br>
			    						<font color="#1B58B8">Speech Communication. 2021 </font><br> </font>
			    						<font color="red">(Top journal, CCF-B, IF=2.723)</font><br>
			    						<font color="#1B58B8"><a href="https://arxiv.org/abs/2105.14762"
			    								target="_blank">[PDF]</a> <a
			    								href="https://hltsingapore.github.io/ESD/demo.html">[DEMO]</a></font>
			    					</li>
			    					<li>
			    						<b>Expressive TTS Training with Frame and Style Reconstruction Loss </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Guanglai Gao, Haizhou Li.<br>
			    						<font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing
			    							(IEEE/ACM-TASLP). 2021 <br></font>
			    						<font color="red">(Top journal, JCR Q1, IF=4.364)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9420276"
			    								target="_blank">[PDF]</a> <a
			    								href="https://ttslr.github.io/Expressive-TTS-Training-with-Frame-and-Style-Reconstruction-Loss/">[DEMO]</a>
			    							<a href="./papers/TASLP2021-style.txt" target="_blank">[BIB]</a>
			    						</font>
			    					</li>
			    					<li>
			    						<b>FastTalker: A Neural Text-to-Speech Architecture with Shallow and Group
			    							Autoregression </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Yixing Lin, Haizhou Li.<br>
			    						<font color="#1B58B8">Neural Networks. 2021 </font>
			    						<br>
			    						<font color="red">(JCR Q1, IF=9.657)</font>
			    						<br>
			    						<font color="#1B58B8"><a
			    								href="https://www.sciencedirect.com/science/article/pii/S0893608021001532"
			    								target="_blank">[PDF]</a> <a
			    								href="https://ttslr.github.io/FastTalker/">[DEMO]</a> <a
			    								href="./papers/NN2021.txt" target="_blank">[BIB]</a> </font>
			    					</li>
			    					<li>
			    						<b>Exploiting Morphological and Phonological Features to Improve Prosodic Phrasing for
			    							Mongolian Speech Synthesis </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Feilong Bao, Jichen Yang, Guanglai Gao, Haizhou Li.<br>
			    						<font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing
			    							(IEEE/ACM-TASLP). 2021 <br></font>
			    						<font color="red">(Top journal, JCR Q1, IF=4.364)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="papers/TASLP2020Mongolian.pdf">[PDF]</a> <a
			    								href="./papers/TASLP2020Mongolian.txt" target="_blank">[BIB]</a></font>
			    					</li>
			    					<li>
			    						<b>Modeling Prosodic Phrasing with Multi-Task Learning in Tacotron-based TTS </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Feilong Bao, Guanglai Gao, Haizhou Li.<br>
			    						<font color="#1B58B8">IEEE Signal Processing Letters. 2020 </font>
			    						<br>
			    						<font color="red">(JCR Q1, IF=3.201)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9166626"
			    								target="_blank">[PDF]</a> <a href="./papers/SPL2020.txt"
			    								target="_blank">[BIB]</a> <a href="https://ttslr.github.io/SPL2020/">[DEMO]</a>
			    						</font>
			    					</li>
			    				</ol>
			    
			    				<h3>Conference papers</h3>
			    				<ol class="paper-list" id="grants">


								<li>
			    						<b>Emotion Rendering for Conversational Speech Synthesis with Heterogeneous Graph-Based Context Modeling </b> <br>
			    						<b>Rui Liu</b>, Yifan Hu, Yi Ren, Xiang Yin, Haizhou Li.<br>
			    						<font color="#1B58B8">38th AAAI Conference on Artificial Intelligence (AAAI'2024).</font> <br>
			    						<font color="red">(Top conference, CCF-A)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/29833"
			    								target="_blank">[PDF]</a> <a
			    								href="https://github.com/walker-hyf/ECSS">[CODE & DEMO]</a></font>
			    					</li>
								
			    					<li>
			    						<b>Exploiting Modality-Invariant Feature for Robust Multimodal Emotion Recognition with
			    							Missing Modalities </b> <br>
			    						Haolin Zuo, <b>Rui Liu <sup>*</sup></b>, Jinming Zhao, Guanglai Gao, Haizhou Li.<br>
			    						<font color="#1B58B8">2023 IEEE International Conference on Acoustics, Speech and Signal
			    							Processing (ICASSP'2023).</font> <br>
			    						<font color="red">(Top conference, CCF-B)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://arxiv.org/abs/2210.15359"
			    								target="_blank">[PDF]</a> <a
			    								href="https://github.com/ZhuoYulang/IF-MMIN">[CODE]</a></font>
			    					</li>
			    					<li>
			    						<b>MnTTS: An Open-Source Mongolian Text-to-Speech Synthesis Dataset and Accompanied
			    							Baseline </b> <br>
			    						Yifan Hu <sup>#</sup>, Pengkai Yin <sup>#</sup>, <b>Rui Liu <sup>*</sup></b>, Feilong
			    						Bao and Guanglai Gao.<br>
			    						<font color="#1B58B8">2022 International Conference on Asian Language Processing
			    							(IALP'2022) </font> <br>
			    						<font color="#1B58B8"><a href="https://arxiv.org/abs/2209.10848"
			    								target="_blank">[PDF]</a> <a
			    								href="https://github.com/walker-hyf/MnTTS">[CODE]</a> <a
			    								href="http://mglip.com/corpus/corpus_detail.html?corpusid=20220819185345">[Application
			    								Entry]</a></font>
			    					</li>
			    					<li>
			    						<b>A Deep Investigation of RNN and Self-attention for the Cyrillic-Traditional Mongolian
			    							Bidirectional Conversion </b> <br>
			    						Muhan Na, <b>Rui Liu <sup>*</sup></b>, Feilong Bao and Guanglai Gao.<br>
			    						<font color="#1B58B8">29th International Conference on Neural Information Processing
			    							(ICONIP'2022) </font> <br>
			    						<font color="red">(CCF-C)</font> <br>
			    						<font color="#1B58B8"><a href="https://arxiv.org/abs/2209.11963"
			    								target="_blank">[PDF]</a> </font>
			    					</li>
			    					<li>
			    						<b>Accurate Emotion Strength Assessment for Seen and Unseen Speech Based on Data-Driven
			    							Deep Learning </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Björn Schuller, Guanglai Gao and Haizhou Li.<br>
			    						<font color="#1B58B8">23th Annual Conference of the International Speech Communication
			    							Association (INTERSPEECH'2022) </font> <br>
			    						<font color="red">(Top conference, CCF-C)</font> <br>
			    						<font color="#1B58B8"><a
			    								href="https://www.isca-speech.org/archive/interspeech_2022/liu22i_interspeech.html"
			    								target="_blank">[PDF]</a> <a
			    								href="https://github.com/ttslr/StrengthNet">[CODE]</a></font>
			    					</li>
			    					<li>
			    						<b>Alignment-Learning based single-step decoding for accurate and fast
			    							non-autoregressive speech recognition</b> <br>
			    						Yonghe Wang, <b>Rui Liu <sup>*</sup></b>, Feilong Bao, Hui Zhang, Guanglai Gao.<br>
			    						<font color="#1B58B8">2022 IEEE International Conference on Acoustics, Speech and Signal
			    							Processing (ICASSP'2022).</font> <br>
			    						<font color="red">(Top conference, CCF-B)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9746227"
			    								target="_blank">[PDF]</a> </font>
			    					</li>
			    					<li>
			    						<b>VisualTTS: TTS with Accurate Lip-speech Synchronization for Automatic Voice Over</b>
			    						<br>
			    						Junchen Lu, Berrak Sisman, <b>Rui Liu</b>, Mingyang Zhang, Haizhou Li.<br>
			    						<font color="#1B58B8">2022 IEEE International Conference on Acoustics, Speech and Signal
			    							Processing (ICASSP'2022).</font> <br>
			    						<font color="red">(Top conference, CCF-B)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://arxiv.org/abs/2110.03342"
			    								target="_blank">[PDF]</a> <a
			    								href="https://ranacm.github.io/VisualTTS-Samples/">[DEMO]</a></font>
			    					</li>
			    					<li>
			    						<b>Mongolian emotional speech synthesis based on transfer learning and emotional
			    							embedding </b> <br>
			    						Aihong Huang, Feilong Bao, Guanglai Gao, Yu Shan, <b>Rui Liu <sup>*</sup></b><br>
			    						<font color="red"><b>(Best Paper Award)</b></font><br>
			    						<font color="#1B58B8">2021 International Conference on Asian Language Information
			    							Processing (IALP'2021) </font> <br>
			    						<font color="#1B58B8"><a href="xx" target="_blank">[PDF]</a> </font>
			    					</li>
			    					<li>
			    						<b>Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion
			    							Discriminability </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Haizhou Li.<br>
			    						<font color="#1B58B8">22th Annual Conference of the International Speech Communication
			    							Association (INTERSPEECH'2021) </font> <br>
			    						<font color="red">(Top conference, CCF-C)</font> <br>
			    						<font color="#1B58B8"><a
			    								href="https://www.isca-speech.org/archive/interspeech_2021/liu21p_interspeech.html"
			    								target="_blank">[PDF]</a> <a href="https://ttslr.github.io/i-ETTS/">[DEMO]</a>
			    							<a href="./papers/InterSpeech2021.txt" target="_blank">[BIB]</a>
			    						</font>
			    					</li>
			    					<li>
			    						<b> GraphSpeech: Syntax-aware Graph Attention Network for Neural Speech Synthesis </b>
			    						<br>
			    						<b>Rui Liu</b>, Berrak Sisman, Haizhou Li.<br>
			    						<font color="#1B58B8">2021 IEEE International Conference on Acoustics, Speech and Signal
			    							Processing (ICASSP'2021), Oral.</font> <br>
			    						<font color="red">(Top conference, CCF-B)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://arxiv.org/pdf/2010.12423.pdf"
			    								target="_blank">[PDF]</a> <a
			    								href="https://ttslr.github.io/GraphSpeech/">[DEMO]</a> <a
			    								href="./papers/ICASSP2021.txt" target="_blank">[BIB]</a> <a
			    								href="https://www.bilibili.com/video/BV16U4y1t7cR/">[VIDEO]</a></font>
			    					</li>
			    					<li>
			    						<b>Seen and Unseen Emotional Style Transfer for Voice Conversion with a New Emotion
			    							Speech Dataset </b> <br>
			    						Kun Zhou, Berrak Sisman, <b>Rui Liu</b>, Haizhou Li.<br>
			    						<font color="#1B58B8">2021 IEEE International Conference on Acoustics, Speech and Signal
			    							Processing (ICASSP'2021), Oral.</font> <br>
			    						<font color="red">(Top conference, CCF-B)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://arxiv.org/pdf/2010.14794.pdf"
			    								target="_blank">[PDF]</a> <a href="./papers/ICASSP2021-Kun.txt"
			    								target="_blank">[BIB]</a> </font>
			    					</li>
			    					<li>
			    						<b>Teacher-Student Training For Robust Tacotron-based TTS </b> <br>
			    						<strong>Rui Liu</strong>, Berrak Sisman, Jingdong Li, Feilong Bao, Guanglai Gao, Haizhou
			    						Li.<br>
			    						<font color="#1B58B8">2020 IEEE International Conference on Acoustics, Speech and Signal
			    							Processing (ICASSP'2020), Oral. <br>
			    							<font color="red">(Top conference, CCF-B) (With Travel Grant)</font>
			    						</font><br>
			    						<font color="#1B58B8"><a href="papers/ICASSP2020.pdf">[PDF]</a> <a
			    								href="./papers/ICASSP2020.txt" target="_blank">[BIB]</a> <a
			    								href="https://ttslr.github.io/ICASSP2020/">[DEMO]</a> <a
			    								href="https://youtu.be/D6gTTuiDdPU">[VIDEO]</a></font>
			    					</li>
			    					<li>
			    						<b>WaveTTS: Tacotron-based TTS with Joint Time-Frequency Domain Loss </b> <br>
			    						<strong>Rui Liu</strong>, Berrak Sisman, Feilong Bao, Guanglai Gao, Haizhou Li.<br>
			    						<font color="#1B58B8">The Speaker and Language Recognition Workshop 2020 (Odyssey'2020).
			    						</font>
			    						<br>
			    						<font color="#1B58B8"><a href="papers/Odyssey2020.pdf">[PDF]</a> <a
			    								href="./papers/Odyssey2020.txt" target="_blank">[BIB]</a> <a
			    								href="https://ttslr.github.io/WaveTTS/">[DEMO]</a> <a href="xxx">[VIDEO]</a>
			    						</font>
			    					</li>
			    					<li>
			    						<b>NUS-HLT System for Blizzard Challenge 2020 </b> <br>
			    						Yi Zhou, Xiaohai Tian, Xuehao Zhou, Mingyang Zhang, Grandee Lee, <strong>Rui
			    							Liu</strong>, Berrak Sisman, and Haizhou Li<br>
			    						<font color="#1B58B8">Joint Workshop for the Blizzard Challenge and Voice Conversion
			    							Challenge 2020 (BC'2020).</font>
			    						<br>
			    						<font color="#1B58B8"><a href="papers/BC2020.pdf">[PDF]</a> <a
			    								href="./papers/BC2020.txt" target="_blank">[BIB]</a> </font>
			    					</li>
			    					<li>
			    						<b>The IMU Speech Synthesis Entry for Blizzard Challenge 2019 </b> <br>
			    						<strong>Rui Liu</strong>, Jingdong Li, Feilong Bao and Guanglai Gao.<br>
			    						<font color="#1B58B8">Blizzard Challenge Workshop 2019 (BC'2019).</font>
			    						<br>
			    						<font color="#1B58B8"><a href="papers/BC2019.pdf">[PDF]</a> <a
			    								href="./papers/BC2019.txt" target="_blank">[BIB]</a> </font>
			    					</li>
			    					<li>
			    						<b>Improving Mongolian Phrase Break Prediction by Using Syllable and Morphological
			    							Embeddings with BiLSTM Model </b> <br>
			    						<strong>Rui Liu</strong>, Feilong Bao, Guanglai Gao, Hui Zhang and Yonghe Wang.<br>
			    						<font color="#1B58B8">19th Annual Conference of the International Speech Communication
			    							Association (INTERSPEECH'2018), Oral </font>
			    						<br>
			    						<font color="red">(Top conference, CCF-C)</font>
			    						<br>
			    						<font color="#1B58B8"><a
			    								href="https://www.isca-speech.org/archive_v0/Interspeech_2018/abstracts/1706.html">[PDF]</a>
			    							<a href="./papers/INTERSPEECH2018.txt" target="_blank">[BIB]</a>
			    						</font>
			    					</li>
			    					<li>
			    						<b>A LSTM Approach with Sub-word Embeddings for Mongolian Phrase Break Prediction </b>
			    						<br>
			    						<strong>Rui Liu</strong>, Feilong Bao, Guanglai Gao, Hui Zhang and Yonghe Wang.<br>
			    						<font color="#1B58B8">27th International Conference on Computational Linguistics
			    							(COLING'2018).</font>
			    						<br>
			    						<font color="red">(Top conference, CCF-B)</font><br>
			    						<font color="#1B58B8"><a href="https://aclanthology.org/C18-1207/">[PDF]</a> <a
			    								href="./papers/COLING2018.txt" target="_blank">[BIB]</a> </font>
			    					</li>
			    					<li>
			    						<b>End-to-End Mongolian Text-to-Speech System </b> <br>
			    						Jingdong Li, Hui Zhang, <strong>Rui Liu</strong>, Xueliang Zhang and Feilong Bao.<br>
			    						<font color="#1B58B8">11th International Symposium on Chinese Spoken Language Processing
			    							(ISCSLP'2018).</font><br>
			    						<font color="#1B58B8"><a href="papers/ISCSLP2018.pdf">[PDF]</a> <a
			    								href="./papers/ISCSLP2018.txt" target="_blank">[BIB]</a></font>
			    					</li>
			    					<li>
			    						<b>Mongolian Text-to-Speech System Based on Deep Neural Network </b> <br>
			    						<strong>Rui Liu</strong>, Feilong Bao, Guanglai Gao and Yonghe Wang.<br>
			    						<font color="#1B58B8">14th National Conference on Man-Machine Speech Communication
			    							(NCMMSC'2017), Oral.</font>
			    						<br>
			    						<font color="#1B58B8"><a href="papers/NCMMSC2017.pdf">[PDF]</a> <a
			    								href="./papers/NCMMSC2017.txt" target="_blank">[BIB]</a> </font>
			    					</li>
			    				</ol>
			     -->
			    				<h2>Projects</h2>
			    				<h3>Principal Investigator</h3>
			    				<ol class="paper-list" id="grants">

									<li>
									Research on Object Detection Methods for Smart Border Defense Applications, 
									High-Level Talent Research Start-up Fund of Inner Mongolia University, 
									CNY 4.2 million, Oct 2024–Oct 2027, Principal Investigator.
									</li>

									<li>
									Research on Object and Terrain Detection Methods on Mars Surface Based on Deep Convolutional Neural Networks, 
									Scientific Research Innovation Fund of Harbin Institute of Technology, 
									Grant No. 2022FRFK02041, CNY 0.1 million, Apr 2022–Apr 2023, Principal Investigator.
									</li>

									<li>
									Box Leakage Detection System, 
									Capital Aerospace Machinery Co., Ltd. (Factory 211), 
									Project No. MH20240136, CNY 0.7 million, Jan 2024–Jan 2025, Principal Investigator.
									</li>

									<li>
									Mass Measurement System, 
									Capital Aerospace Machinery Co., Ltd. (Factory 211), 
									Project No. MH20240188, CNY 0.15 million, Jan 2024–Jan 2025, Principal Investigator.
									</li>

									<li>
									Ground Simulation Equipment for Motion Mechanism, 
									Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Sciences, 
									Project No. MH20231460, CNY 0.145 million, Aug 2023–Aug 2024, Principal Investigator.
									</li>

									<li>
									Full-Process Simulation System for Jupiter Exploration Mission, 
									Shanghai Institute of Satellite Engineering, 
									Project No. MH20231379, CNY 0.55 million, Dec 2023–Dec 2024, Principal Investigator.
									</li>

									<li>
									AR Glasses Development, 
									Jilin Juhong Intelligent Technology Co., Ltd., 
									Project No. MH20220659, CNY 0.2 million, Jul 2022–Jan 2024, Principal Investigator.
									</li>

									<li>
									Teaching Management System (QZQY) Development, 
									Aerospace New Long March Avenue Technology Co., Ltd., 
									Project No. MH20220658, CNY 0.41 million, Jul 2022–Jan 2024, Principal Investigator.
									</li>

									<li>
									Development of Fault Injection System, 
									Beijing Aerospace Automatic Control Institute, 
									Project No. MH20210708, CNY 0.29 million, Jul 2021–Jul 2022, Principal Investigator.
									</li>

									<li>
									Simulation and Development of Electromagnetic Pulse Effects on Typical Electronic Equipment, 
									Beijing Aerospace Automatic Control Institute, 
									Project No. MH20200641, CNY 0.3 million, Sep 2020–Sep 2021, Principal Investigator.
									</li>

			    					
								
			    				</ol>
			    				
							
<!-- 							<h3>Co-Principal Investigator</h3>
			    				<ol class="paper-list" id="grants">
			    					<li>
			    						Coming soon...
			    					</li>
			    				</ol>
			     -->
			    
			    				<!-- <h2>Talks</h2>
			    				<ol class="paper-list" id="grants">
			    					<li>
			    						<b>Title: </b><u>Mongolian Text-to-Speech Technology</u> （蒙古语语音合成技术）. <br />[<a
			    							href="./slides/多语种论坛-蒙古语TTS-v2.ppt" target="_blank">Slides</a>]
			    						[<a href="https://mp.weixin.qq.com/s/tdyJ0dygEwysYmGrfBFbxA" target="_blank">Video</a>]
			    						<br />
			    						<b>Organizer:</b> Chinese Association for Artificial Intelligence （CAAI） <br />
			    						<b>Date:</b> 20 Aug 2022
			    					</li>
			    					<li>
			    						<b>Title: </b><u>Emotion Intensity Research of Speech Synthesis</u> (语音合成中的情感强度建模研究).
			    						<br />[<a href="./slides/语音之家-情感强度建模-PPT.pdf" target="_blank">Slides</a>]
			    						[<a href="https://appzxw56sw27444.h5.xiaoeknow.com/v2/course/alive/l_6281eb95e4b0cedf38b26577?app_id=appzxw56sw27444&pro_id=&type=2&available=true&share_user_id=u_6278bea47e200_rUccGC7f14&share_type=5&scene=%E5%88%86%E4%BA%AB&is_redirect=1&share_scene=1&entry=2&entry_type=2002"
			    							target="_blank">Video</a>]
			    						<br />
			    						<b>Organizer:</b> SpeechHome （语音之家） <br />
			    						<b>Date:</b> 19 May 2022
			    					</li>
			    					<li>
			    						<b>Title: </b><u>Prosody and Emotion Modeling in End-to-End Speech Synthesis
			    						</u>（端到端语音合成中的韵律、情感建模研究）. <br />[<a href="./slides/CCF专委会报告--语音合成韵律情感-刘瑞.pdf"
			    							target="_blank">Slides</a>]
			    						[<a href="https://mp.weixin.qq.com/s/O5ok2Uh0Sd639C5oqtRo8A"
			    							target="_blank">Video</a>]<br />
			    						<b>Organizer:</b> CCF Professional Committee of Speech Dialogue and Auditory Processing
			    						<br />
			    						<b>Date:</b> 04 Dec 2021
			    					</li>
			    				</ol> -->
			    
			    				<h2>Major Appointments</h2>
			    				<ol class="paper-list" id="grants">
								<li>Member, IEEE.</li>

								<li>Reviewer, IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI).</li>

								<li>Reviewer, IEEE Transactions on Neural Networks and Learning Systems (IEEE TNNLS).</li>

								<li>Reviewer, Pattern Recognition (PR).</li>

								<li>Reviewer, IEEE Signal Processing Letters (IEEE SPL).</li>

								<li>Reviewer, ACM Transactions on Intelligent Systems and Technology (ACM TIST).</li>

								<li>Reviewer, Journal of Visual Communication and Image Representation (JVCI).</li>

								<li>Reviewer, IET Image Processing (IET-IPR).</li>

								<li>Reviewer, IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</li>

								<li>Reviewer, International Conference on Computer Vision (ICCV).</li>

								<li>Reviewer, European Conference on Computer Vision (ECCV).</li>

								<li>Reviewer, AAAI Conference on Artificial Intelligence (AAAI).</li>

								<li>Reviewer, Asian Conference on Computer Vision (ACCV).</li>

								<li>Reviewer, Chinese Conference on Pattern Recognition and Computer Vision (PRCV).</li>

								<li>Reviewer, Winter Conference on Applications of Computer Vision (WACV).</li>	
								
<!-- 			    				<li><b>Conference Reviewer</b>:<br>
									- ACL 2024 <br>
									- NAACL 2024  <br>
			    						- INTERSPEECH 2021/2022/2023/2024<br>
			    						- ICASSP 2021/2022/2023/2024<br>
			    						- Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020<br>
			    						- SLT 2022<br>
			    						- O-COCOSDA 2022<br>
			    					<li><b>Journal Reviewer</b>: <br>
			    						- IEEE/ACM Transactions on Audio, Speech, and Language Processing (IEEE/ACM-TASLP) <br>
									- IEEE Transactions on Neural Networks and Learning Systems (IEEE-TNNLS)<br>
			    						- IEEE Signal Processing Letters<br>
			    						- IEEE Internet of Things Journal (IEEE-IoTJ)<br>
									- Neural Nertworks<br>
									- NeuroComputing<br>
									- Speech Communication<br> -->
			    					
			    				</ol>
			    
<!-- 			    				<h2>Awards</h2>
			    				<ol class="paper-list" id="grants">
								<li>Apr 2023, <a href="https://www.acmturc.com/2022_bak/cn/mobile/new_star_award.html"
			    							target="_blank"> 2022 ACM China Rising Star</a> (Hohhot Chapter), ACM (Association for Computing Machinery) China Council
			    					<li>Dec 2021, Excellent Doctoral dissertation of Inner Mongolia Autonomous Region</li>
			    					<li>Dec 2021, IALP-2021 <font color="red"><b>Best Paper</b></font>
			    					</li>
			    					<li>July 2021, <a href="https://www.acmturc.com/2021/en/doctoral_thesis_award.html"
			    							target="_blank"> 2020 ACM China Doctoral Dissertation Award </a> (Hohhot Chapter),
			    						ACM (Association for Computing Machinery) China Council </li>
			    					<li>Sep 2020, Excellent Doctoral dissertation of Inner Mongolia University</li>
			    					<li>Feb 2020, ICASSP IEEE SPS Travel Grant </li>
			    					<li>Aug 2019, Research Scholarship of China Scholarship Council (CSC) </li>
			    					<li>Oct 2018, National scholarship for Doctoral students (top 2% students), Ministry of
			    						Education of P.R.China </li>
			    					<li>Oct 2018, Academic scholarship of Inner Mongolia autonomous region </li>
			    					<li>Oct 2017, National scholarship for Doctoral students (top 2% students), Ministry of
			    						Education of P.R.China </li>
			    					<li>Oct 2017, Academic scholarship of Inner Mongolia autonomous region </li>
			    					<li>Oct 2016, Academic scholarship of Inner Mongolia autonomous region </li>
			    					 
			    				</ol> -->
			    
			
			    				
<!-- 			    				<h2>More about Me</h2> -->
			    			</section>
			    			<footer class="page__meta"></footer>
			    		</div>
			    	</article>
			    </div>
				<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5bbihrdqns5&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
			</div>
			
			<div class="module" id="module2" style="display: none;">
			    <div class="block">
			    		<span>Principal Investigator</span>
			    		<div class="people_list">
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/YongqiangZhang.jpg"/>
			    				</div>
			    				<span>Yongqiang Zhang <br/> (张永强)</span><br/> <br/>
			    				<!-- <span><underline><a href="https://ttslr.github.io/index_ruiliu.html"  onclick="showModule(1)">Personal Homepage</a></underline></span> -->
			    				<span><underline><a onclick="showModule(1)">Personal Homepage</a></underline></span>
			    				<!-- <span>Rui Liu is a professor in Department of Computer Science at Inner Mongolia University of China, who is leading the S2LAB.</span> -->
			    			</div>
			    		</div>
			    	</div>
			    	
			    	<!-- 博士 -->
			    	<div class="block">
			    		<span>PhD Student</span>
			    	
			    		<span>24级</span>
			    		<div class="people_list">
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/lqy.jpg"/>
			    				</div>
			    				<span>Qiuyu Liang <br/> (梁秋雨) '24<br/>[开放词汇目标检测]</span> <br/><br/><br/>
			    				
			    			</div>
			    		</div>
			    		<span>25级</span>
			    		<div class="people_list">
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/gyh.jpg"/>
			    				</div>
			    				<span>Yuhan Gao <br/> (高语涵) '25<br/>[域泛化目标检测]</span><br/><br/><br/>
		
			    			</div>

							<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/wn.jpg"/>
			    				</div>
			    				<span>Nan Wang <br/> (王楠) '25<br/>[图像超分辨率]</span><br/><br/><br/>
		
			    			</div>

							<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/zy.jpg"/>
			    				</div>
			    				<span>Yuan Zhao <br/> (赵源) '25<br/>[指代音视频分割]</span><br/><br/><br/>
		
			    			</div>
			    			

			    		</div>
						
	
			    	</div>
			    	
			    	<!-- 硕士 -->
			    	
			    	<div class="block">
			    		<span>Master Student</span>
			    		 
<!-- 			    		<span>22级</span>
			    		<div class="people_list" style="justify-content: flex-start;">
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/lb.jpg"/>
			    				</div>
			    				<span>Bin Liu <br/> (刘彬) '学硕<br/>[韵律预测]</span>
			    			</div>
			    			
			    			
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/lh.jpg"/>
			    				</div>
			    				<span>Huan Liu <br/> (刘欢) '学硕<br/>[语音情感识别]</span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/xjt.jpg"/>
			    				</div>
			    				<span>Jiatian Xi <br/> (席嘉甜) '专硕<br/>[语音编辑]</span><br/><br/><br/>
			    				<span  style="color: #015293;"><b>(与 浙江大学 联合指导)</b></span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/mzn.jpg"/>
			    				</div>
			    				<span>Zening Ma <br/> (马泽宁) '专硕<br/>[自监督预训练模型]</span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/lkl.jpg"/>
			    				</div>
			    				<span>Kailin Liang <br/> (梁凯麟) '专硕<br/>[语音情感迁移]</span>
			    			</div>
			    			
			    			 
			    			
			    		</div> -->
			    		<span>24级</span>
			    		<div class="people_list" style="justify-content: flex-start;">

							<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/ayk.jpg"/>
			    				</div>
			    				<span>Yuke Ai <br/> (艾宇科) '学硕<br/>[目标检测]</span>
			    			</div>

							<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/ckp.jpg"/>
			    				</div>
			    				<span>Kaipeng Chen <br/> (陈凯鹏) '学硕<br/>[开放词汇目标检测]</span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/wt.jpg"/>
			    				</div>
			    				<span>Tao Wu <br/> (吴涛) '专硕<br/>[多目标跟踪]</span>
			    			</div>


			    			
			    			
			    		
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/lcl.jpg"/>
			    				</div>
			    				<span>Changlin Lu <br/> (路昌林) '专硕<br/>[小目标检测]</span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/xxx.jpg"/>
			    				</div>
			    				<span>Zhili Liu <br/> (刘治理) '专硕<br/>[xxx]</span>
			    			</div>
			    			
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/xxx.jpg"/>
			    				</div>
			    				<span>xxxx <br/> (赵文桢) '专硕<br/>[xxxx]</span>
			    			</div>
			    			

			    			
			    		</div>
			    		
						<span>25级</span>
						<div class="people_list" style="justify-content: flex-start;">
							
							<div class="people">
								<div class="people_header">
									<img src="assets/S2Group_members/fzh.jpg"/>
								</div>
								<span>Zhihao Fu <br/> (付志豪) '学硕<br/>[域泛化目标检测]</span>
							</div>
							
							<div class="people">
								<div class="people_header">
									<img src="assets/S2Group_members/ly.jpg"/>
								</div>
								<span>Yu Liu <br/> (刘玉) '学硕<br/>[全模态指代分割]</span>
							</div>
							
							<div class="people">
								<div class="people_header">
									<img src="assets/S2Group_members/qzm.jpg"/>
								</div>
								<span>xxxx <br/> (邱子萌) '学硕<br/>[弱监督领域自适应]</span>
							</div>
						
							<div class="people">
								<div class="people_header">
									<img src="assets/S2Group_members/ljh.jpg"/>
								</div>
								<span>Jiahui Li <br/> (李佳桧) '学硕<br/>[目标追踪]</span>
							</div>
							
							<div class="people">
								<div class="people_header">
									<img src="assets/S2Group_members/wjr.jpg"/>
								</div>
								<span>Jiarui Wang <br/> (王嘉锐) '专硕<br/>[伪装目标检测]</span>
							</div>
							
							<div class="people">
								<div class="people_header">
									<img src="assets/S2Group_members/dwz.jpg"/>
								</div>
								<span>Wenzhi Dong <br/> (董文智) '专硕<br/>[开放词汇目标检测]</span>
							</div>

							<div class="people">
								<div class="people_header">
									<img src="assets/S2Group_members/qzy.jpg"/>
								</div>
								<span>Zhenyuan Qi <br/> (齐振源) '专硕<br/>[真实世界图像超分]</span>
							</div>

							<div class="people">
								<div class="people_header">
									<img src="assets/S2Group_members/rxx.jpg"/>
								</div>
								<span>Xiaoxu Ren <br/> (任晓旭) '专硕<br/>[密集检测]</span>
							</div>
							
						</div>
						
						
			    	</div>
			
			    	
			</div>
			
			<div class="module" id="module3" style="display: none;">
				<!-- <div class="block" style="text-align: center;">Coming Soon...</div> -->
				<div class="block">
					<div class="collaborators-list">
					    <div class="collaborator">
					        <img src="assets/collaborators/zwm.jpg" alt="Collaborator 1" class="collaborator-image">
					        <div class="collaborator-info">
					            <h2>左旺孟</h2>
					            <p>左旺孟，哈尔滨工业大学计算机学院长聘教授、博士生导师。长期关注元学习、迁移学习和生成式对抗网络模型方法及其在底层视觉、图像生成、视觉跟踪、物体检测和图像分类等领域的应用。在CVPR/ICCV/ECCV等顶级会议和T-PAMI/IJCV和IEEE Trans.等顶级期刊上发表论文200余篇，谷歌学术引用87,000余次。提出的DnCNN模型被正式收入MATLAB 2017b及后续版本的Image Processing和Deep Learning Toolbox，并被CV-News Magazine作为Main Story专题报道。担任人工智能学会模式识别专委会常委、中国图象图形学会机器视觉专委会常委、中国图像图形学会青工委执委。多次受邀担任ICCV、CVPR等CCF-A类领域主席及IJCAI、AAAI等CCF-A类会议高级程序委员。担任IEEE TPAMI和IEEE TIP等期刊的AE。</p>
							</div>
					    </div>
				
					    <!-- 可以根据需要继续添加更多合作者 -->
					</div>
				</div>
				
			</div>
		</main>
		<footer class="pc_footer">
			<div class="element">
				<i class="fa fa-envelope-open-o" aria-hidden="true" /></i>
				<span>zhangyongqiang@imu.edu.cn</span>
			</div>
			
			<div class="element">
				<i class="fa fa-phone" aria-hidden="true" /></i>
				<span>+86 15045651571</span>
			</div>
			
			<div class="element">
				<i class="fa fa-map-marker" aria-hidden="true" /></i>
				<span> 406 Room, School of Computer science, <br/>Inner Mongolia University (010021)</span>
			</div>
			
			<div class="element">
				<i class="fa fa-map" aria-hidden="true" /></i>
				<span>Hohhot, China</span>
			</div>
		</footer>
		<script src="./js/main.min.js"></script>
		<script>
			(function(i, s, o, g, r, a, m) {
				i['GoogleAnalyticsObject'] = r;
				i[r] = i[r] || function() {
						(i[r].q = i[r].q || []).push(arguments)
					},
					i[r].l = 1 * new Date();
				a = s.createElement(o),
					m = s.getElementsByTagName(o)[0];
				a.async = 1;
				a.src = g;
				m.parentNode.insertBefore(a, m)
			})(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
			ga('create', '', 'auto');
			ga('send', 'pageview');
			
		
			
			// 为第一个module 增加 加粗效果
			document.addEventListener('DOMContentLoaded', function() {
			    // 获取第一个菜单项元素
			    var firstMenuItem = document.querySelector('.masthead__menu-item');
			
			    // 为第一个菜单项添加 masthead__menu-item--lg 类
			    if (firstMenuItem) {
			        firstMenuItem.classList.add('masthead__menu-item--lg');
			    }
			});
		
			
		
			function showModule(moduleNumber) {
			    // 获取所有模块元素
			    var modules = document.querySelectorAll('.module');
			
			    // 隐藏所有模块
			    modules.forEach(function(module) {
			        module.style.display = 'none';
			    });
			
			    // 显示指定模块
			    var targetModule = document.getElementById('module' + moduleNumber);
			    if (targetModule) {
			        targetModule.style.display = 'block';
			    }
			
			    // 清除所有菜单项的活动状态
			    var menuItems = document.querySelectorAll('.masthead__menu-item');
			    menuItems.forEach(function(item) {
			        item.classList.remove('masthead__menu-item--lg');
			    });
			
			    // 将点击的 li 元素添加 masthead__menu-item--lg 类
			    var targetMenuItem = document.querySelector('.masthead__menu-item:nth-child(' + moduleNumber + ')');
			    if (targetMenuItem) {
			        targetMenuItem.classList.add('masthead__menu-item--lg');
			    }
			}
		
		</script>
	</body>
</html>

